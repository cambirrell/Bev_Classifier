{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bev Classifier Model\n",
    "Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import crop\n",
    "from torchvision.transforms import Compose, Resize, Lambda, ToTensor\n",
    "import warnings\n",
    "# Suppress the specific UserWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"The default value of the antialias parameter.*\", category=UserWarning)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test datasets\n",
    "#train and test datasets are split across a few folders but contain test or train in their name\n",
    "class BevDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='/mnt/pccfs2/backed_up/cambirrell/bev_classification/images', split='train', transform=None, subset=None, step_one=False):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.subset = subset\n",
    "        self.step_one = step_one\n",
    "        # #use a reg ex to find all the files in the root directory that contain the word train or test\n",
    "        # self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if split in f]\n",
    "        #if train is spesified, open all folders from train_0 to train_69 and add the images in their subdirectories to the files list\n",
    "        if split == 'train':\n",
    "            for i in range(70):\n",
    "                self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root + '/train_' + str(i)) for f in filenames]\n",
    "        #if test is spesified, open all folders from test_0 to test_14 and add the images in their subdirectories to the files list\n",
    "        if split == 'test':\n",
    "            for i in range(15):\n",
    "                self.files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root + '/test_' + str(i)) for f in filenames]\n",
    "        # print(len(self.files))\n",
    "        #remove any files that are not .jpg\n",
    "        self.files = [f for f in self.files if '.jpg' in f]\n",
    "        \n",
    "\n",
    "        #\n",
    "        #if a subset is specified, then only add the subdirectory that contains the subset name\n",
    "        if self.subset:\n",
    "            cat = json.load(open('catagories.json'))\n",
    "            valid = cat[self.subset]\n",
    "            self.files = [f for f in self.files if os.path.basename(os.path.dirname(f)) in valid]\n",
    "        #set the labels based on the subdirectory name\n",
    "        self.labels = [os.path.basename(os.path.dirname(f)) for f in self.files]\n",
    "        #if step_one is true, read the catagories .json file and set the labels to the key of the value where the current label is an element of the value\n",
    "        \n",
    "        if step_one:\n",
    "            self.labels = [self.get_label(f) for f in self.labels]\n",
    "        else:\n",
    "            self.cat = json.load(open('catagories.json'))\n",
    "            self.labels = [self.get_label_step_two(f) for f in self.labels]\n",
    "            \n",
    "    def get_label(self, f):\n",
    "            cat = json.load(open('catagories.json'))\n",
    "            for k, v in cat.items():\n",
    "                if f in v:\n",
    "                    return k\n",
    "    def get_label_step_two(self, f):\n",
    "        valid = cat[self.subset]\n",
    "        return valid.index(f)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #open the image and apply the transform\n",
    "        img = Image.open(self.files[idx])\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        #return the image and the label\n",
    "        self.labels = [int(i) for i in self.labels]\n",
    "        if self.subset and not self.step_one:\n",
    "            cat = json.load(open('catagories.json'))\n",
    "            valid = cat[self.subset]\n",
    "            return img, F.one_hot(torch.tensor(self.labels[idx]), len(valid)).float() \n",
    "        return img, F.one_hot(torch.tensor(self.labels[idx]), 17).float() if self.step_one else self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "train_dataset = BevDataset(transform=transform, split='train', step_one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the train and test datasets and apply the transforms to make sure it all works\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "train_dataset = BevDataset(transform=transform, split='train', step_one=True)\n",
    "# test_dataset = BevDataset(transform=transform, split='test')\n",
    "data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for x, y in train_dataset:\n",
    "    print(x.shape, y.shape) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a resnet that takes in 3 channel images then outputs 17 classes\n",
    "class ResNetStepOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetStepOne, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 17)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an googleNet that takes in 3 channel images then outputs 17 classes\n",
    "class GoogleNetStepOne(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogleNetStepOne, self).__init__()\n",
    "        self.googlenet = models.googlenet(pretrained=True)\n",
    "        self.googlenet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.googlenet.fc = nn.Linear(1024, 17)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.googlenet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 224, 224)\n",
    "test_model = GoogleNetStepOne()\n",
    "test_model(a.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make 16 resnets that take in 3 channel images then the amount of output classes are the amount of element in the items for each key in the categories.json file\n",
    "class ResNetStepTwo(nn.Module):\n",
    "    def __init__(self, output_classes):\n",
    "        super(ResNetStepTwo, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, output_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkae a vgg that takes in 3 channel images then outputs 17 classes\n",
    "class VGGStepTwo(nn.Module):\n",
    "    def __init__(self, output_classes):\n",
    "        super(VGGStepTwo, self).__init__()\n",
    "        self.vgg = models.vgg16(pretrained=True)\n",
    "        self.vgg.features[0] = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.vgg.classifier[6] = nn.Linear(4096, output_classes)\n",
    "        # print(\"Output classes: \", output_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vgg(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step one training\n",
    "def train_step_one(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    loop = tqdm(total=len(train_loader)*epochs, position=0, leave=False)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            #make y a one hot vector\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            loop.update(1)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                predicted = torch.argmax(y_hat, 1)\n",
    "                actual = torch.argmax(y, 1) \n",
    "                total += y.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "        print(f'Epoch {epoch}, Validation Accuracy: {correct / total}')\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one training test\n",
    "dataset = BevDataset(transform=transform, split='train', step_one=True)\n",
    "train_size = int(0.8 * len(dataset)) \n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_data = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "model = GoogleNetStepOne()\n",
    "train_step_one(model, data, val_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step two training - train the expert models\n",
    "#train all 16 models at once by spreading the data across the gpus\n",
    "def train_step_two(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    # loop = tqdm(total=len(train_loader)*epochs, position=0, leave=False)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            #make y a one hot vector\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            print(y_hat.shape, y.shape)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # loop.set_description(f\"Epoch [{epoch}/{epochs}]\")\n",
    "            # loop.set_postfix(loss=loss.item())\n",
    "            # loop.update(1)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                predicted = torch.argmax(y_hat, 1)\n",
    "                actual = torch.argmax(y, 1) \n",
    "                total += y.size(0)\n",
    "                correct += (predicted == actual).sum().item()\n",
    "    print(f'Epoch {epoch}, Validation Accuracy: {correct / total}')\n",
    "    model.to('cpu')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagories = json.load(open('catagories.json'))\n",
    "expert_models = {k: VGGStepTwo(len(v)) for k, v in catagories.items()}\n",
    "for k, v in expert_models.items():\n",
    "    print(\"Catagory:\", k)\n",
    "    dataset = BevDataset(transform=transform, split='train', subset=k, step_one=False)\n",
    "    train_size = int(0.85 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_data = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    expert_models[k] = train_step_two(expert_models[k], data, val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation, use the trained models to predict the labels of the test data\n",
    "#run throught the step one model and then depending on the output, run through the corresponding step two model\n",
    "#return top 1 and top 5 accuracy and put all the results in a dataframe and then save it to a txt file\n",
    "def evaluate(model, test_loader, expert):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top_5_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            y_hats = [expert[k](x) for k in catagories.keys()]\n",
    "            y_hat = torch.cat(y_hats, dim=1)\n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            top_5_correct += (predicted == y).sum().item()\n",
    "    top_1_accuracy = correct / total\n",
    "    top_5_accuracy = top_5_correct / total\n",
    "    results = pd.DataFrame({'Model': [model.__class__.__name__], 'Top 1 Accuracy': [top_1_accuracy], 'Top 5 Accuracy': [top_5_accuracy]})\n",
    "    results.to_csv('results.txt', mode='a', header=False, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(BevDataset(split='train', transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(BevDataset(split='val', transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(BevDataset(split='test', transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])), batch_size=64, shuffle=True)\n",
    "train_step_one(ResNetStepOne(), train_loader, val_loader, epochs=10, lr=1e-3)\n",
    "train_step_two(expert_models, train_loader, val_loader, epochs=10, lr=1e-3)\n",
    "evaluate(ResNetStepOne(), test_loader, expert_models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
